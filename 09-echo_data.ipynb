{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal echoing\n",
    "\n",
    " - Practicum: [Week6](https://www.youtube.com/watch?v=8cAffg2jaT0) - [Instant](https://youtu.be/8cAffg2jaT0?t=3019) \n",
    "\n",
    "\n",
    "Echoing signal `n` steps is an example of synchronized many-to-many task. \n",
    "\n",
    "Echoing signal means the network has to remember previous states. In this case, we are going to use a simple RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.sequential_tasks import EchoData\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset\n",
    "\n",
    "To generate the dataset we are going to use the utility [`EchoData`](https://github.com/Atcold/pytorch-Deep-Learning/blob/6a8a86013de961acea1d1fd6dc0c712db607974d/res/sequential_tasks.py#L75).\n",
    "\n",
    "This utility generates (`batch_size`) sequences of (`series_length`) binary digits each one. The expected output is move (`echo_step`) positions to the right. In the first position we introduce (`echo_step`) random digits. \n",
    "\n",
    "For example, to this toy sequence:\n",
    "\n",
    "```\n",
    "x = [1 0 1 1 1 0 0]\n",
    "```\n",
    "\n",
    "the expected response with 2 echo steps is:\n",
    "\n",
    "```\n",
    "y = [0 0 1 0 1 1 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 \n",
    "echo_step = 3 # We use a short number of echoing steps\n",
    "series_length = 20_000 \n",
    "BPTT_T = 20\n",
    "\n",
    "train_data = EchoData(\n",
    "    echo_step=echo_step,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    truncated_length=BPTT_T,\n",
    ")\n",
    "train_size = len(train_data)\n",
    "\n",
    "test_data = EchoData(\n",
    "    echo_step=echo_step,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    truncated_length=BPTT_T,\n",
    ")\n",
    "test_size = len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`len` function in a `EchoData` object returns the number of available baches. Number of baches is equal to `series_length/T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(train_size)\n",
    "print(int(series_length/BPTT_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print first 20 timesteps of the first sequences to see the echo data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1st input sequence)  x: 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 ... \n",
      "(1st target sequence) y: 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 ... \n"
     ]
    }
   ],
   "source": [
    "print('(1st input sequence)  x:', *train_data.x_batch[0, :20], '... ')\n",
    "print('(1st target sequence) y:', *train_data.y_batch[0, :20], '... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch_size different sequences are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch:\n",
      "1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 ...\n",
      "0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 ...\n",
      "1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 ...\n",
      "0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 ...\n",
      "0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 ...\n",
      "x_batch size: (5, 20000)\n",
      "\n",
      "y_batch:\n",
      "0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 ...\n",
      "0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 ...\n",
      "0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 ...\n",
      "0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 ...\n",
      "0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 ...\n",
      "y_batch size: (5, 20000)\n"
     ]
    }
   ],
   "source": [
    "print('x_batch:', *(str(d)[1:-1] + ' ...' for d in train_data.x_batch[:, :20]), sep='\\n')\n",
    "print('x_batch size:', train_data.x_batch.shape)\n",
    "print()\n",
    "print('y_batch:', *(str(d)[1:-1] + ' ...' for d in train_data.y_batch[:, :20]), sep='\\n')\n",
    "print('y_batch size:', train_data.y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use RNNs data is organized into temporal chunks of size `[batch_size, T, feature_dim]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_chunk:\n",
      "[1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1]\n",
      "[0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1]\n",
      "[1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0]\n",
      "[0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0]\n",
      "[0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0]\n",
      "1st x_chunk size: (5, 20, 1)\n",
      "\n",
      "y_chunk:\n",
      "[0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1]\n",
      "[0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0]\n",
      "[0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0]\n",
      "[0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1]\n",
      "1st y_chunk size: (5, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_chunk:', *train_data.x_chunks[0].squeeze(), sep='\\n')\n",
    "print('1st x_chunk size:', train_data.x_chunks[0].shape)\n",
    "print()\n",
    "print('y_chunk:', *train_data.y_chunks[0].squeeze(), sep='\\n')\n",
    "print('1st y_chunk size:', train_data.y_chunks[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Recurrent Neural Network (RNN) model\n",
    "\n",
    "In this case, we need a model that remember what was the last processed information (we echoing the previous step). RNN gives us some capacity of remember. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, rnn_hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=1,\n",
    "            nonlinearity='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(\n",
    "            in_features=rnn_hidden_size,\n",
    "            out_features=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.rnn(x, hidden)  \n",
    "        x = self.linear(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training routine. Remember all the training process includes the following 5 steps:\n",
    " 1. Forward process: `output = model(data)`\n",
    " 2. Get loss: `loss = criterion(output, target)`\n",
    " 3. Client gradient buffers: `optimizer.zero_grad()`\n",
    " 4. Calculate gradient (the partial derivate of the loss with the respect the network paramenters): `loss.backward()`\n",
    " 5. Perform training setp (step in the oppositional direction of the gradient): `optimizer.step()`\n",
    "\n",
    "If some of these steps are missed the training is going to go wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hidden):\n",
    "    \n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "       \n",
    "    # Store the number of sequences that were classified correctly\n",
    "    correct = 0\n",
    "    \n",
    "    # Iterate over every batch of sequences.\n",
    "    for batch_idx in range(train_size):\n",
    "        \n",
    "        # Request a batch of sequences and class labels, convert them into tensors\n",
    "        # of the correct type, and then send them to the appropriate device.\n",
    "        data, target = train_data[batch_idx]\n",
    "        data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).float().to(device)\n",
    "        \n",
    "         # Clear the gradient buffers of the optimized parameters.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if hidden is not None: hidden.detach_()\n",
    "        \n",
    "        # Perform the forward pass of the model\n",
    "        logits, hidden = model(data, hidden)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(logits, target)\n",
    "        \n",
    "        # Calculate gradient \n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform training setp (\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute if the prediction was correct. In particular, if the values of each digit is greater than 0.5 is\n",
    "        # a 1 and if their are less than 0.5 is a 0.\n",
    "        pred = (torch.sigmoid(logits) > 0.5)\n",
    "        correct += (pred == target.byte()).int().sum().item()\n",
    "        \n",
    "    return correct, loss.item(), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Testing Loop\n",
    "\n",
    "In testing it is important to avoid the gradient calculations during inference: `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(hidden):\n",
    "    \n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()   \n",
    "    \n",
    "    # Store the number of sequences that were classified correctly\n",
    "    correct = 0\n",
    "    \n",
    "    # Avoid the gradient calculations during inference\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Iterate over every batch of sequences\n",
    "        for batch_idx in range(test_size):\n",
    "            \n",
    "            # Request a batch of sequences and class labels, convert them into tensors\n",
    "            # of the correct type, and then send them to the appropriate device.\n",
    "            data, target = test_data[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).float().to(device)\n",
    "            \n",
    "            # Perform the forward pass of the model\n",
    "            logits, hidden = model(data, hidden)\n",
    "            \n",
    "            # Compute if the prediction was correct.\n",
    "            pred = (torch.sigmoid(logits) > 0.5)\n",
    "            correct += (pred == target.byte()).int().sum().item()\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 1 #since we have a scalar series\n",
    "h_units = 4\n",
    "\n",
    "model = SimpleRNN(\n",
    "    input_size=1,\n",
    "    rnn_hidden_size=h_units,\n",
    "    output_size=feature_dim\n",
    ").to(device)\n",
    "hidden = None\n",
    "        \n",
    "# BCEWithLogitsLoss: This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "#  - https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# RMSprop - https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/5, loss: 0.488, accuracy 56.8%\n",
      "Train Epoch: 2/5, loss: 0.052, accuracy 87.0%\n",
      "Train Epoch: 3/5, loss: 0.001, accuracy 99.9%\n",
      "Train Epoch: 4/5, loss: 0.000, accuracy 100.0%\n",
      "Train Epoch: 5/5, loss: 0.000, accuracy 100.0%\n",
      "Test accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "epoch = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    correct, loss, hidden = train(hidden)\n",
    "    epoch += 1\n",
    "    train_accuracy = float(correct) / train_size\n",
    "    print(f'Train Epoch: {epoch}/{n_epochs}, loss: {loss:.3f}, accuracy {train_accuracy:.1f}%')\n",
    "\n",
    "#test    \n",
    "correct = test(hidden)\n",
    "test_accuracy = float(correct) / test_size\n",
    "print(f'Test accuracy: {test_accuracy:.1f}%')\n",
    "\n",
    "## Depends on the initialization of the network the model can't reach or not the maximum accuracy.\n",
    "## Re-lunch this cell if you don't get the 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the model with a random sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: \n",
      "tensor([[1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "         0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "         0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         1, 1, 0, 0]], device='cuda:0', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# Generate the input sequence\n",
    "my_input = torch.empty(1, 100, 1).random_(2).to(device)\n",
    "print(\"Input sequence: \")\n",
    "print(my_input.view(1, -1).byte())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the sequence\n",
    "hidden = None\n",
    "my_out, _ = model(my_input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Echoing sequence:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the result\n",
    "print(\"Echoing sequence:\")\n",
    "response = (my_out > 0).view(1, -1).to(int)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the echoing was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct echoing digits from the 97 : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_result = my_input.detach().view(1, -1).to(int).cpu().numpy()\n",
    "expected_result = np.append(expected_result[:,-3:], expected_result[:,0:-3]).astype(int)\n",
    "\n",
    "obtained_result = (my_out > 0).view(1, -1).to(int).cpu().numpy()\n",
    "\n",
    "print(f\"Number of correct echoing digits from the {len(expected_result[3:])} : \")\n",
    "(obtained_result[0][3:] == expected_result[3:]).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN is valid to process with no so much needed memory. \n",
    "\n",
    "For the main example, the number of echoing steps was little. **What happens when the RNN model has to learn echoing more steps?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a new dataset with more echoing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 \n",
    "echo_step = 10 # We increase the number of echoing steps\n",
    "series_length = 20_000 \n",
    "BPTT_T = 20\n",
    "\n",
    "train_data = EchoData(\n",
    "    echo_step=echo_step,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    truncated_length=BPTT_T,\n",
    ")\n",
    "train_size = len(train_data)\n",
    "\n",
    "test_data = EchoData(\n",
    "    echo_step=echo_step,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    truncated_length=BPTT_T,\n",
    ")\n",
    "test_size = len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Re-use the same model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 1 #since we have a scalar series\n",
    "h_units = 4\n",
    "\n",
    "model = SimpleRNN(\n",
    "    input_size=1,\n",
    "    rnn_hidden_size=h_units,\n",
    "    output_size=feature_dim\n",
    ").to(device)\n",
    "hidden = None\n",
    "        \n",
    "# BCEWithLogitsLoss: This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "#  - https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# RMSprop - https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the training (with some more epochs to help the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/20, loss: 0.693, accuracy 49.7%\n",
      "Train Epoch: 2/20, loss: 0.693, accuracy 49.9%\n",
      "Train Epoch: 3/20, loss: 0.694, accuracy 50.1%\n",
      "Train Epoch: 4/20, loss: 0.692, accuracy 50.2%\n",
      "Train Epoch: 5/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 6/20, loss: 0.694, accuracy 49.6%\n",
      "Train Epoch: 7/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 8/20, loss: 0.693, accuracy 50.3%\n",
      "Train Epoch: 9/20, loss: 0.693, accuracy 50.0%\n",
      "Train Epoch: 10/20, loss: 0.694, accuracy 50.0%\n",
      "Train Epoch: 11/20, loss: 0.693, accuracy 49.9%\n",
      "Train Epoch: 12/20, loss: 0.694, accuracy 50.4%\n",
      "Train Epoch: 13/20, loss: 0.693, accuracy 50.2%\n",
      "Train Epoch: 14/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 15/20, loss: 0.693, accuracy 49.9%\n",
      "Train Epoch: 16/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 17/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 18/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 19/20, loss: 0.693, accuracy 50.0%\n",
      "Train Epoch: 20/20, loss: 0.693, accuracy 50.2%\n",
      "Test accuracy: 49.9%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20 # We increase the number of \n",
    "epoch = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    correct, loss, hidden = train(hidden)\n",
    "    epoch += 1\n",
    "    train_accuracy = float(correct) / train_size\n",
    "    print(f'Train Epoch: {epoch}/{n_epochs}, loss: {loss:.3f}, accuracy {train_accuracy:.1f}%')\n",
    "\n",
    "#test    \n",
    "correct = test(hidden)\n",
    "test_accuracy = float(correct) / test_size\n",
    "print(f'Test accuracy: {test_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN cann't learn the echoing sequence because with a long echoing sequence it needs more memory.\n",
    "\n",
    "**What happens when we increase the number of hidden units? Does it increase the RNN memory?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 1 #since we have a scalar series\n",
    "h_units = 60 ## We increase the number of units\n",
    "\n",
    "model = SimpleRNN(\n",
    "    input_size=1,\n",
    "    rnn_hidden_size=h_units,\n",
    "    output_size=feature_dim\n",
    ").to(device)\n",
    "hidden = None\n",
    "        \n",
    "# BCEWithLogitsLoss: This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "#  - https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# RMSprop - https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/20, loss: 0.693, accuracy 50.2%\n",
      "Train Epoch: 2/20, loss: 0.693, accuracy 49.8%\n",
      "Train Epoch: 3/20, loss: 0.694, accuracy 49.8%\n",
      "Train Epoch: 4/20, loss: 0.693, accuracy 50.2%\n",
      "Train Epoch: 5/20, loss: 0.692, accuracy 50.1%\n",
      "Train Epoch: 6/20, loss: 0.694, accuracy 49.8%\n",
      "Train Epoch: 7/20, loss: 0.693, accuracy 49.9%\n",
      "Train Epoch: 8/20, loss: 0.694, accuracy 49.9%\n",
      "Train Epoch: 9/20, loss: 0.693, accuracy 50.2%\n",
      "Train Epoch: 10/20, loss: 0.693, accuracy 49.9%\n",
      "Train Epoch: 11/20, loss: 0.693, accuracy 50.2%\n",
      "Train Epoch: 12/20, loss: 0.694, accuracy 50.0%\n",
      "Train Epoch: 13/20, loss: 0.693, accuracy 49.9%\n",
      "Train Epoch: 14/20, loss: 0.693, accuracy 50.2%\n",
      "Train Epoch: 15/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 16/20, loss: 0.693, accuracy 49.9%\n",
      "Train Epoch: 17/20, loss: 0.692, accuracy 50.1%\n",
      "Train Epoch: 18/20, loss: 0.693, accuracy 50.0%\n",
      "Train Epoch: 19/20, loss: 0.694, accuracy 49.9%\n",
      "Train Epoch: 20/20, loss: 0.693, accuracy 50.0%\n",
      "Test accuracy: 49.8%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20 # We increase the number of \n",
    "epoch = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    correct, loss, hidden = train(hidden)\n",
    "    epoch += 1\n",
    "    train_accuracy = float(correct) / train_size\n",
    "    print(f'Train Epoch: {epoch}/{n_epochs}, loss: {loss:.3f}, accuracy {train_accuracy:.1f}%')\n",
    "\n",
    "#test    \n",
    "correct = test(hidden)\n",
    "test_accuracy = float(correct) / test_size\n",
    "print(f'Test accuracy: {test_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that increasing the number of hidden units doesn't help to increase the long-term memory.\n",
    "\n",
    "**What happens if we added more hidden layers? Does it increase the RNN memory?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongerRNN(nn.Module):\n",
    "    def __init__(self, input_size, rnn_hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=6,\n",
    "            nonlinearity='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(\n",
    "            in_features=rnn_hidden_size,\n",
    "            out_features=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.rnn(x, hidden)  \n",
    "        x = self.linear(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 1 #since we have a scalar series\n",
    "h_units = 60 ## We increase the number of units\n",
    "\n",
    "model = LongerRNN(\n",
    "    input_size=1,\n",
    "    rnn_hidden_size=h_units,\n",
    "    output_size=feature_dim\n",
    ").to(device)\n",
    "hidden = None\n",
    "        \n",
    "# BCEWithLogitsLoss: This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "#  - https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# RMSprop - https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/20, loss: 0.690, accuracy 49.9%\n",
      "Train Epoch: 2/20, loss: 0.693, accuracy 50.2%\n",
      "Train Epoch: 3/20, loss: 0.694, accuracy 49.9%\n",
      "Train Epoch: 4/20, loss: 0.694, accuracy 50.1%\n",
      "Train Epoch: 5/20, loss: 0.691, accuracy 50.1%\n",
      "Train Epoch: 6/20, loss: 0.694, accuracy 49.9%\n",
      "Train Epoch: 7/20, loss: 0.693, accuracy 49.9%\n",
      "Train Epoch: 8/20, loss: 0.693, accuracy 50.0%\n",
      "Train Epoch: 9/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 10/20, loss: 0.694, accuracy 49.8%\n",
      "Train Epoch: 11/20, loss: 0.692, accuracy 49.9%\n",
      "Train Epoch: 12/20, loss: 0.694, accuracy 50.0%\n",
      "Train Epoch: 13/20, loss: 0.693, accuracy 49.9%\n",
      "Train Epoch: 14/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 15/20, loss: 0.692, accuracy 50.2%\n",
      "Train Epoch: 16/20, loss: 0.692, accuracy 49.9%\n",
      "Train Epoch: 17/20, loss: 0.693, accuracy 50.1%\n",
      "Train Epoch: 18/20, loss: 0.693, accuracy 50.0%\n",
      "Train Epoch: 19/20, loss: 0.695, accuracy 50.0%\n",
      "Train Epoch: 20/20, loss: 0.693, accuracy 50.0%\n",
      "Test accuracy: 49.8%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20 # We increase the number of \n",
    "epoch = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    correct, loss, hidden = train(hidden)\n",
    "    epoch += 1\n",
    "    train_accuracy = float(correct) / train_size\n",
    "    print(f'Train Epoch: {epoch}/{n_epochs}, loss: {loss:.3f}, accuracy {train_accuracy:.1f}%')\n",
    "\n",
    "#test    \n",
    "correct = test(hidden)\n",
    "test_accuracy = float(correct) / test_size\n",
    "print(f'Test accuracy: {test_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither that works. **How can we improve the long-term memory of a RNN?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pDL] *",
   "language": "python",
   "name": "conda-env-.conda-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
