{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    " - Practicum: [Week12](https://www.youtube.com/watch?v=f01J0Dri-6k) - [Instant](https://youtu.be/f01J0Dri-6k?t=2530)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "\n",
    "This lesson can be a bit dense and difficult to follow. At the same time, I have special interest in learn about Transformers. \n",
    "\n",
    "For these reasons, in this notebook, I describe and complete the calculations and formulations presented in the video. In particular, I break down each equation and I represent each vector and matrix. \n",
    "\n",
    "I changed a bit Alfredo's annotation to make it easier to represent and understand by me. In any case, they are minor changes and they are easy to identify.\n",
    "\n",
    "The following explanations follow the structure of the video and slides. You can combine the sources to a better understanding.\n",
    "\n",
    "*GitHub users: the matrixes and other mathematical formulation are not properly visualized in the GitHub's view. For a properly visualization please download the repository (to include images) and load the notebook with Jupiter notebook.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "Input set of size `t`: $\\{x^{(i)}\\}_{i=1}^{t} = \\{x^{(1)}, x^{(2)}, ..., x^{(t)}\\}$ , `t` elements\n",
    " \n",
    "Each $x^{(i)}$  €  $\\mathbb{R}^n$  = \\{$x_1^{(i)}$, $x_2^{(i)}$, ..., $x_n^{(i)}$\\} , embedding of `n` features\n",
    "\n",
    "![input-repres](res/transformers_xinput_embbd.png)\n",
    "\n",
    "If we concat the `t` examples together we get a matrix:\n",
    "\n",
    " $X$ € $\\mathbb{R}^{nxt} = \\begin{pmatrix}x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & ... & x_1^{(t)}\\\\\\ \n",
    "                 x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & ... & x_2^{(t)}\\\\\\ \n",
    "                 ... \\\\\\ \n",
    "                 x_n^{(1)} & x_n^{(2)} & x_n^{(3)} & ... & x_n^{(t)}\n",
    "\\end{pmatrix}$ , `n` rows (embedding) and `t` columns (inputs)\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden representation\n",
    "\n",
    "Using a self-atention you try to get a hidden representation: $h$ &rarr; linear combination of the input vectors\n",
    "\n",
    "$h = X*a$ &rarr; h € $\\mathbb{R}^{n}$\n",
    "\n",
    "\n",
    "$ \n",
    "h = \n",
    "\\begin{pmatrix}\n",
    "    x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & ... & x_1^{(t)}\\\\\\ \n",
    "    x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & ... & x_2^{(t)}\\\\\\ \n",
    "    ... \\\\\\ \n",
    "    x_n^{(1)} & x_n^{(2)} & x_n^{(3)} & ... & x_n^{(t)}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "    a_1\\\\\\\n",
    "    a_2\\\\\\\n",
    "    ... \\\\\\\n",
    "    a_t\n",
    "\\end{pmatrix} = \n",
    "  a_1 \\begin{pmatrix}x_1^{(1)}\\\\\\ x_2^{(1)}\\\\\\ ... \\\\\\ x_n^{(1)}\\end{pmatrix}\n",
    "+ a_2 \\begin{pmatrix}x_1^{(2)}\\\\\\ x_2^{(2)}\\\\\\ ... \\\\\\ x_n^{(2)}\\end{pmatrix} \n",
    "+ ... \n",
    "+ a_t \\begin{pmatrix}x_1^{(t)}\\\\\\ x_2^{(t)}\\\\\\ ... \\\\\\ x_n^{(t)}\\end{pmatrix}  = \n",
    "\\begin{pmatrix}\n",
    "    h_1 \\\\\\ \n",
    "    h_2\\\\\\ \n",
    "    ... \\\\\\ \n",
    "    h_n\n",
    "\\end{pmatrix}$ \n",
    "\n",
    "\n",
    "&rarr; $a$ € $\\mathbb{R}^{t} = \\begin{pmatrix}a_1\\\\\\ a_2\\\\\\ ... \\\\\\ a_t\\end{pmatrix}$\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "$a$ is the attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Types of attention\n",
    "We have 2 kinds of attention:\n",
    "\n",
    "<br/>\n",
    "\n",
    "  - *hard attention* \n",
    "\n",
    "$||a||_0 = 1$, the zero norm and the non-zero term are equals to 1 &rarr; one-hot encoded vector\n",
    "\n",
    "example, $a = \\begin{pmatrix} 0\\\\\\ 1\\\\\\ 0\\\\\\ ... \\\\\\ 0\\end{pmatrix}$\n",
    "\n",
    "When you use a one-hot vector $h$ only pays attention to only one column of $X$, only one example, $x^{(i)}$, the rest are zero.\n",
    " \n",
    "example, $ h = \\begin{pmatrix}x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & ... & x_1^{(t)}\\\\\\ \n",
    "                 x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & ... & x_2^{(t)}\\\\\\ \n",
    "                 ... \\\\\\ \n",
    "                 x_n^{(1)} & x_n^{(2)} & x_n^{(3)} & ... & x_n^{(t)}\n",
    "\\end{pmatrix} \\begin{pmatrix}0\\\\\\ 1\\\\\\ 0 \\\\\\ ... \\\\\\ 0 \\end{pmatrix}$ = $ 0 \\begin{pmatrix}x_1^{(1)}\\\\\\ x_2^{(1)}\\\\\\ ... \\\\\\ x_n^{(1)}\\end{pmatrix} + 1 \\begin{pmatrix}x_1^{(2)}\\\\\\ x_2^{(2)}\\\\\\ ... \\\\\\ x_n^{(2)}\\end{pmatrix} + 0 \\begin{pmatrix}x_1^{(3)}\\\\\\ x_2^{(3)}\\\\\\ ... \\\\\\ x_n^{(3)}\\end{pmatrix} + ... + 0 \\begin{pmatrix}x_1^{(t)}\\\\\\ x_2^{(t)}\\\\\\ ... \\\\\\ x_n^{(t)}\\end{pmatrix} $ = $\\begin{pmatrix}\n",
    "0 & x_1^{(2)} & 0 & ... & 0\\\\\\ \n",
    "0 & x_2^{(2)} & 0 & ... & 0\\\\\\ \n",
    "... \\\\\\ \n",
    "0 & x_n^{(2)} & 0 & ... & 0\n",
    "\\end{pmatrix}$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    " \n",
    "  - *soft attention* \n",
    "\n",
    "$||a||_1 = 1$, the summation of the elements of $a$ is 1 &rarr; probability vector\n",
    "\n",
    "example, $a = \\begin{pmatrix} p_1\\\\\\ p_2\\\\\\ p_3\\\\\\ ... \\\\\\ p_t\\end{pmatrix}$ => $\\sum_{j=1}^{t} p_j = 1$\n",
    "\n",
    "In this case, your hidden representation is a weighted combination of columns, it pays attention to all the examples $x^{(i)}$ but it puts more attention to ones to the others. \n",
    " \n",
    " \n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention calculation\n",
    "\n",
    "How `attention`, $a$, is calculated:\n",
    "\n",
    "![attention_equation](res/transformers_attention_eq.png)\n",
    "\n",
    "You can use:\n",
    "\n",
    "   [argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html): it gets the index of the max value of the vector &rarr; one-hot vector &rarr; `hard attention`\n",
    "   \n",
    "   \n",
    "   [softargmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html): it obtains the probability distribution &rarr; `soft attention`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the equation is $X^Tx$; \n",
    "\n",
    "With: $X^T$ € $\\mathbb{R}^{txn}$; $x$ € $\\mathbb{R}^{n}$ &rarr; $X^Tx$ € $\\mathbb{R}^{t}$\n",
    "\n",
    "$ X^Tx = \n",
    "\\begin{pmatrix}x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & ... & x_n^{(1)}\\\\\\ \n",
    "               x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & ... & x_n^{(3)}\\\\\\ \n",
    "               ... \\\\\\ \n",
    "               x_1^{(t)} & x_2^{(t)} & x_3^{(t)} & ... & x_n^{(t)}\n",
    "\\end{pmatrix}  \n",
    "\\begin{pmatrix}x_1^{'}\\\\\\ x_2^{'}\\\\\\ ... \\\\\\ x_n^{'}\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix} \\alpha_1 \\\\\\ \\alpha_2\\\\\\ ... \\\\\\ \\alpha_t\\end{pmatrix}\n",
    "$\n",
    "\n",
    "> This multiplication, $X^Tx$, compute how aline is $x$ with the input dataset, or in other words, **how similar is $x$ with each element of the dataset $X$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "We have a set of $x$'s: $\\{x^{(i)}\\}_{i=1}^{t} = \\{x^{(1)}, x^{(2)}, ..., x^{(t)}\\}$ , `t` elements\n",
    "\n",
    "That implies you have a set of $a$'s: $\\{x^{(i)}\\}_{i=1}^{t}$  &rarr;  $\\{a^{(i)}\\}_{i=1}^{t}$\n",
    "\n",
    "In other words, you can apply `attention` to each $x^{(i)}$ of the set, and get the corresponding $a^{(i)}$.\n",
    "\n",
    "Generalization, $\\{a^{(i)}\\}_{i=1}^{t}$; $a$ € $\\mathbb{R}^{t}$  &rarr; $A$ € $\\mathbb{R}^{txt}$\n",
    "\n",
    "$A = \\begin{pmatrix}\n",
    "          a_1^{(1)} & a_1^{(2)} ... & a_1^{(t)} \\\\\\\n",
    "          a_2^{(1)} & a_2^{(2)} ... & a_2^{(t)} \\\\\\ \n",
    "          ... \\\\\\\n",
    "          a_t^{(1)} & a_t^{(2)} ... & a_t^{(t)} \n",
    "     \\end{pmatrix} $\n",
    "     \n",
    "It has a dimension of $txt$ because you have $t$ examples and for example, each example feature is the measure of `attention` of this $x$ with respect to each of the $x$'s of the dataset.\n",
    " \n",
    "<br/> \n",
    "\n",
    "\n",
    "Then, if we have a set of `attentions` $a$'s we can get a set of `hidden layers` $h$'s: $\\{a^{(i)}\\}_{i=1}^{t}$  &rarr;  $\\{h^{(i)}\\}_{i=1}^{t}$\n",
    "\n",
    "Generalization, $\\{h^{(i)}\\}_{i=1}^{t}$ = $H$ € $\\mathbb{R}^{nxt}$\n",
    "\n",
    "Where, $H = XA$\n",
    "\n",
    "\n",
    "$H = \\begin{pmatrix}\n",
    "        x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & ... & x_1^{(t)}\\\\\\ \n",
    "        x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & ... & x_2^{(t)}\\\\\\ \n",
    "        ... \\\\\\ \n",
    "        x_n^{(1)} & x_n^{(2)} & x_n^{(3)} & ... & x_n^{(t)}\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "        a_1^{(1)} & a_1^{(2)} ... & a_1^{(t)} \\\\\\\n",
    "        a_2^{(1)} & a_2^{(2)} ... & a_2^{(t)} \\\\\\ \n",
    "        ... \\\\\\\n",
    "        a_t^{(1)} & a_t^{(2)} ... & a_t^{(t)} \n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "        h_{(1,1)} & h_{(1,2)} & h_{(1,3)} & ... & h_{(1,t)}\\\\\\ \n",
    "        h_{(2,1)} & h_{(2,2)} & h_{(2,3)} & ... & h_{(2,t)}\\\\\\ \n",
    "        ... \\\\\\ \n",
    "        h_{(n,1)} & h_{(n,2)} & h_{(n,3)} & ... & h_{(n,t)}\n",
    "\\end{pmatrix} \n",
    "$\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-value store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paradigm for storing, retrieving and managing an associative array (dictionary / hash table). In detail, paradigm to recovery *values* ($v$'s) stored by *keys* ($k$'s) using a *query* ($q$'s). \n",
    "\n",
    "| k      | v        |\n",
    "|--------|----------|\n",
    "| key_1  | value_A  |\n",
    "| key_2  | value_B  |\n",
    "| key_3  | value_C  |\n",
    "\n",
    "In this paradigm, the retrieving process is: $max(similarity(q,k))=value$\n",
    "\n",
    "<br/>\n",
    "\n",
    "For example, you have stored a list of videos (values) indexed by their titles (keys) and you search one using keywords (query).\n",
    "\n",
    "\n",
    "| k      | v        |\n",
    "|--------|----------|\n",
    "| Understand ML  | http://youtube.com/video/understanding_ml  |\n",
    "| Messi Best Goals   | http://youtube.com/video/messi_best_goals  |\n",
    "| Practicum: Attention and the Transformer  | http://youtube.com/video/attention_transformer  |\n",
    "| ...  | ...  |\n",
    "\n",
    "Querying by : $q$=`transformers` \n",
    "\n",
    "We recibe the value `http://youtube.com/video/attention_transformer`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries, keys and values\n",
    "\n",
    "We can use the input to calculate queries, keys and values if our goal is related on the transformation of the input set (ex., translate a text to other language).\n",
    "\n",
    "$q = W_qx$ ; $k = W_kx$ ; $v = W_vx$\n",
    "\n",
    "To compare $q$ and $k$ they have to have the same dimension: $q,k$ € $\\mathbb{R}^{d}$\n",
    "\n",
    "The value has its own dimensionality: $v$ € $\\mathbb{R}^{p}$\n",
    "\n",
    "In basis that,\n",
    "\n",
    " $W_q, W_k$ € $\\mathbb{R}^{dxn}$\n",
    " \n",
    " $W_v$ € $\\mathbb{R}^{pxn}$\n",
    "\n",
    "In detail,\n",
    "\n",
    "$ q = W_qx = \n",
    "\\begin{pmatrix}\n",
    "    w_{q(1,1)} & w_{q(1,2)} & w_{q(1,3)} & ... & w_{q(1,n)}\\\\\\ \n",
    "    w_{q(2,1)} & w_{q(2,2)} & w_{q(2,3)} & ... & w_{q(2,n)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{q(d,1)} & w_{q(d,2)} & w_{q(d,3)} & ... & w_{q(d,n)}\\\\\\\n",
    "\\end{pmatrix}  \n",
    "\\begin{pmatrix}\n",
    "    x_1^{'}\\\\\\ x_2^{'}\\\\\\ ... \\\\\\ x_n^{'}\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix} \n",
    "    q_1 \\\\\\ q_2\\\\\\ ... \\\\\\ q_d \n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$ k = W_kx = \n",
    "\\begin{pmatrix}\n",
    "    w_{k(1,1)} & w_{k(1,2)} & w_{k(1,3)} & ... & w_{k(1,n)}\\\\\\ \n",
    "    w_{k(2,1)} & w_{k(2,2)} & w_{k(2,3)} & ... & w_{k(2,n)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{k(d,1)} & w_{k(d,2)} & w_{k(d,3)} & ... & w_{k(d,n)}\\\\\\\n",
    "\\end{pmatrix}  \n",
    "\\begin{pmatrix}\n",
    "    x_1^{'}\\\\\\ x_2^{'}\\\\\\ ... \\\\\\ x_n^{'}\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix} \n",
    "    k_1 \\\\\\ k_2\\\\\\ ... \\\\\\ k_d \n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$ v = W_vx = \n",
    "\\begin{pmatrix}\n",
    "    w_{v(1,1)} & w_{v(1,2)} & w_{v(1,3)} & ... & w_{v(1,n)}\\\\\\ \n",
    "    w_{v(2,1)} & w_{v(2,2)} & w_{v(2,3)} & ... & w_{v(2,n)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{v(p,1)} & w_{v(p,2)} & w_{v(p,3)} & ... & w_{v(p,n)}\\\\\\\n",
    "\\end{pmatrix}  \n",
    "\\begin{pmatrix}\n",
    "    x_1^{'}\\\\\\ x_2^{'}\\\\\\ ... \\\\\\ x_n^{'}\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix} \n",
    "    v_1 \\\\\\ v_2\\\\\\ ... \\\\\\ v_p \n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Vectorization\n",
    "\n",
    "We have a set of $x$'s: $\\{x^{(i)}\\}_{i=1}^{t} = \\{x^{(1)}, x^{(2)}, ..., x^{(t)}\\}$ , `t` elements\n",
    "\n",
    "That implies you have sets of $q$'s, $k$'s and $v$'s: $\\{x^{(i)}\\}_{i=1}^{t}$  &rarr;  $\\{q^{(i)}\\}_{i=1}^{t}; \\{k^{(i)}\\}_{i=1}^{t} ; \\{v^{(i)}\\}_{i=1}^{t} $\n",
    "\n",
    "In general,\n",
    "\n",
    "$\\{q^{(i)}\\}_{i=1}^{t}; q$ € $\\mathbb{R}^{d} $ &rarr; $Q$ € $\\mathbb{R}^{dxt}$\n",
    "\n",
    "$Q = \\begin{pmatrix}\n",
    "        q_1^{(1)} & q_1^{(2)} & q_1^{(3)} & ... & q_1^{(t)}\\\\\\ \n",
    "        q_2^{(1)} & q_2^{(2)} & q_2^{(3)} & ... & q_2^{(t)}\\\\\\ \n",
    "        ... \\\\\\ \n",
    "        q_d^{(1)} & q_d^{(2)} & q_d^{(3)} & ... & q_d^{(t)}\n",
    "\\end{pmatrix} $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$\\{k^{(i)}\\}_{i=1}^{t}; k$ € $\\mathbb{R}^{d} $ &rarr; $K$ € $\\mathbb{R}^{dxt}$\n",
    "\n",
    "$K = \\begin{pmatrix}\n",
    "        k_1^{(1)} & k_1^{(2)} & k_1^{(3)} & ... & k_1^{(t)}\\\\\\ \n",
    "        k_2^{(1)} & k_2^{(2)} & k_2^{(3)} & ... & k_2^{(t)}\\\\\\ \n",
    "        ... \\\\\\ \n",
    "        k_d^{(1)} & k_d^{(2)} & k_d^{(3)} & ... & k_d^{(t)}\n",
    "\\end{pmatrix} $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$\\{v^{(i)}\\}_{i=1}^{t}; v$ € $\\mathbb{R}^{p} $ &rarr; $V$ € $\\mathbb{R}^{pxt}$\n",
    "\n",
    "$V = \\begin{pmatrix}\n",
    "        v_1^{(1)} & v_1^{(2)} & v_1^{(3)} & ... & v_1^{(t)}\\\\\\ \n",
    "        v_2^{(1)} & v_2^{(2)} & v_2^{(3)} & ... & v_2^{(t)}\\\\\\ \n",
    "        ... \\\\\\ \n",
    "        v_p^{(1)} & v_p^{(2)} & v_p^{(3)} & ... & v_p^{(t)}\n",
    "\\end{pmatrix} $\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention calculation\n",
    "\n",
    "Now, if we apply this `key-value` paradigm to the `self-attention` process we have the equation: \n",
    "\n",
    "![cross-attention](res/transformers_cross_attention_eq.png)\n",
    "\n",
    "As before, we can use:\n",
    "\n",
    " - [argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html): it gets the index of the max value of the vector &rarr; one-hot vector &rarr; from a logic perspective, it only retrieves one $value$\n",
    "   \n",
    "   \n",
    " - [softargmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html): it obtains the probability distribution &rarr; from a logic perspective, it retrieves weighted $values$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the equation is $K^Tq$; \n",
    "\n",
    "With: $K^T$ € $\\mathbb{R}^{txd}$; $q$ € $\\mathbb{R}^{d}$ &rarr; $K^Tq$ € $\\mathbb{R}^{t}$\n",
    "\n",
    "$ K^Tq = \n",
    "\\begin{pmatrix}\n",
    "        k_1^{(1)} & k_2^{(1)} & k_3^{(1)} & ... & k_d^{(1)}\\\\\\ \n",
    "        k_1^{(2)} & k_2^{(2)} & k_3^{(2)} & ... & k_d^{(2)}\\\\\\ \n",
    "        ... \\\\\\ \n",
    "        k_1^{(t)} & k_2^{(t)} & k_3^{(t)} & ... & k_d^{(t)}\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix} q_1\\\\\\ q_2\\\\\\ ... \\\\\\ q_d\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix} \\alpha_1 \\\\\\ \\alpha_2\\\\\\ ... \\\\\\ \\alpha_t\\end{pmatrix}\n",
    "$\n",
    "\n",
    "> This multiplication, $K^Tq$, compute how aline is $q$ with each key, or in other words, **how similar is $q$ with each key of the $K$**.\n",
    "\n",
    "In this case, if we apply `softargmax` we have the problem that the magnitud grows with the square root of the number of dimensions.\n",
    "To avoid it, we should use:\n",
    "\n",
    "$ \\beta = 1/ \\sqrt{d} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layer\n",
    "\n",
    "With this attention we can compute the hidden layer:\n",
    "\n",
    "$h = Va$\n",
    "\n",
    "In detail, $V$ € $\\mathbb{R}^{pxt}$ ; $a$ € $\\mathbb{R}^{t}$  &rarr;  $h$ € $\\mathbb{R}^{p}$\n",
    "\n",
    "$h = Va = \\begin{pmatrix}\n",
    "        v_1^{(1)} & v_1^{(2)} & v_1^{(3)} & ... & v_1^{(t)}\\\\\\ \n",
    "        v_2^{(1)} & v_2^{(2)} & v_2^{(3)} & ... & v_2^{(t)}\\\\\\ \n",
    "        ... \\\\\\ \n",
    "        v_p^{(1)} & v_p^{(2)} & v_p^{(3)} & ... & v_p^{(t)}\n",
    "\\end{pmatrix} \\begin{pmatrix}a_1\\\\\\ a_2\\\\\\ ... \\\\\\ a_t\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "    h_1 \\\\\\ \n",
    "    h_2\\\\\\ \n",
    "    ... \\\\\\ \n",
    "    h_p\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "We have sets of $q$'s: $\\{q^{(i)}\\}_{i=1}^{t} = \\{q^{(1)}, q^{(2)}, ..., q^{(t)}\\}$ , `t` elements\n",
    "\n",
    "That implies you have sets of $a$'s: $\\{q^{(i)}\\}_{i=1}^{t}$  &rarr;  $\\{a^{(i)}\\}_{i=1}^{t} $ &rarr; $A$ € $\\mathbb{R}^{txt}$\n",
    "\n",
    "$A = \\begin{pmatrix}\n",
    "          a_1^{(1)} & a_1^{(2)} ... & a_1^{(t)} \\\\\\\n",
    "          a_2^{(1)} & a_2^{(2)} ... & a_2^{(t)} \\\\\\ \n",
    "          ... \\\\\\\n",
    "          a_t^{(1)} & a_t^{(2)} ... & a_t^{(t)} \n",
    "     \\end{pmatrix} $\n",
    "\n",
    " \n",
    "<br/> \n",
    "\n",
    "\n",
    "Then, if we have a set of `attentions` $a$'s we can get a set of `hidden layers` $h$'s: $\\{a^{(i)}\\}_{i=1}^{t}$  &rarr;  $\\{h^{(i)}\\}_{i=1}^{t}$\n",
    "\n",
    "Generalization, $\\{h^{(i)}\\}_{i=1}^{t}$ = $H$ € $\\mathbb{R}^{pxt}$\n",
    "\n",
    "Where, $H = VA$\n",
    "\n",
    "\n",
    "$H = \\begin{pmatrix}\n",
    "        v_1^{(1)} & v_1^{(2)} & v_1^{(3)} & ... & v_1^{(t)}\\\\\\ \n",
    "        v_2^{(1)} & v_2^{(2)} & v_2^{(3)} & ... & v_2^{(t)}\\\\\\ \n",
    "        ... \\\\\\ \n",
    "        v_p^{(1)} & v_p^{(2)} & v_p^{(3)} & ... & v_p^{(t)}\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "        a_1^{(1)} & a_1^{(2)} ... & a_1^{(t)} \\\\\\\n",
    "        a_2^{(1)} & a_2^{(2)} ... & a_2^{(t)} \\\\\\ \n",
    "        ... \\\\\\\n",
    "        a_t^{(1)} & a_t^{(2)} ... & a_t^{(t)} \n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "        h_{(1,1)} & h_{(1,2)} & h_{(1,3)} & ... & h_{(1,t)}\\\\\\ \n",
    "        h_{(2,1)} & h_{(2,2)} & h_{(2,3)} & ... & h_{(2,t)}\\\\\\ \n",
    "        ... \\\\\\ \n",
    "        h_{(p,1)} & h_{(p,2)} & h_{(p,3)} & ... & h_{(p,t)}\n",
    "\\end{pmatrix} \n",
    "$\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "To parallelize the computation we can stick all the matrices in only one matrix:\n",
    "\n",
    "![cross-attetion-imp](res/transformers_cross_attention_implementation_eq.png)\n",
    "\n",
    "This composed matrix is € $\\mathbb{R}^{2d+p}$\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "        q\\\\\\ \n",
    "        k\\\\\\ \n",
    "        v\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "    w_{q(1,1)} & w_{q(1,2)} & w_{q(1,3)} & ... & w_{q(1,n)}\\\\\\ \n",
    "    w_{q(2,1)} & w_{q(2,2)} & w_{q(2,3)} & ... & w_{q(2,n)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{q(d,1)} & w_{q(d,2)} & w_{q(d,3)} & ... & w_{q(d,n)}\\\\\\ \n",
    "    w_{k(1,1)} & w_{k(1,2)} & w_{k(1,3)} & ... & w_{k(1,n)}\\\\\\ \n",
    "    w_{k(2,1)} & w_{k(2,2)} & w_{k(2,3)} & ... & w_{k(2,n)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{k(d,1)} & w_{k(d,2)} & w_{k(d,3)} & ... & w_{k(d,n)}\\\\\\\n",
    "    w_{v(1,1)} & w_{v(1,2)} & w_{v(1,3)} & ... & w_{v(1,n)}\\\\\\ \n",
    "    w_{v(2,1)} & w_{v(2,2)} & w_{v(2,3)} & ... & w_{v(2,n)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{v(p,1)} & w_{v(p,2)} & w_{v(p,3)} & ... & w_{v(p,n)}\\\\\\\n",
    "\\end{pmatrix}  \n",
    "\\begin{pmatrix}\n",
    "    x_1^{'}\\\\\\ x_2^{'}\\\\\\ ... \\\\\\ x_n^{'}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "    q_1 \\\\\n",
    "    q_2 \\\\\n",
    "    ... \\\\\n",
    "    q_d \\\\\n",
    "    k_1 \\\\\n",
    "    k_2 \\\\\n",
    "    ... \\\\\n",
    "    k_d \\\\\n",
    "    v_1 \\\\\n",
    "    v_2 \\\\\n",
    "    ... \\\\\n",
    "    v_p \\\\\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "In this way you can compute all the params in only one iteration.\n",
    "\n",
    "* **Multi-head**\n",
    "\n",
    "This implementation of cross-attention is considered `one` head. We can calculate at same time more than one head. That means, different `attention` values and `hidden representations` to the same `input`.\n",
    "\n",
    "Usually, we consider `h` heads. The output vector is the concatenation of the `h` matrixes/vectors of $q$, $k$ and $v$:\n",
    "\n",
    "![cross-attetion-heads-imp](./res/transformers_cross_attention_imp_heads_eq.png)\n",
    "\n",
    "The final vector is € $\\mathbb{R}^{h(2d+p)}$\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "        q^{(1)}\\\\\\ \n",
    "        ...\\\\\\ \n",
    "        q^{(h)}\\\\\\ \n",
    "        k^{(1)}\\\\\\ \n",
    "        ...\\\\\\ \n",
    "        k^{(h)}\\\\\\ \n",
    "        v^{(1)}\\\\\\ \n",
    "        ...\\\\\\ \n",
    "        v^{(h)}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "    w_{q(1,1)}^{(1)} & w_{q(1,2)}^{(1)} & w_{q(1,3)}^{(1)} & ... & w_{q(1,n)}^{(1)}\\\\\\ \n",
    "    w_{q(2,1)}^{(1)} & w_{q(2,2)}^{(1)} & w_{q(2,3)}^{(1)} & ... & w_{q(2,n)}^{(1)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{q(d,1)}^{(1)} & w_{q(d,2)}^{(1)} & w_{q(d,3)}^{(1)} & ... & w_{q(d,n)}^{(1)}\\\\\\ \n",
    "    ... ... ... \\\\\\\n",
    "    w_{q(1,1)}^{(h)} & w_{q(1,2)}^{(h)} & w_{q(1,3)}^{(h)} & ... & w_{q(1,n)}^{(h)}\\\\\\ \n",
    "    w_{q(2,1)}^{(h)} & w_{q(2,2)}^{(h)} & w_{q(2,3)}^{(h)} & ... & w_{q(2,n)}^{(h)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{q(d,1)}^{(h)} & w_{q(d,2)}^{(h)} & w_{q(d,3)}^{(h)} & ... & w_{q(d,n)}^{(h)}\\\\\\ \n",
    "    w_{k(1,1)}^{(1)} & w_{k(1,2)}^{(1)} & w_{k(1,3)}^{(1)} & ... & w_{k(1,n)}^{(1)}\\\\\\ \n",
    "    w_{k(2,1)}^{(1)} & w_{k(2,2)}^{(1)} & w_{k(2,3)}^{(1)} & ... & w_{k(2,n)}^{(1)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{k(d,1)}^{(1)} & w_{k(d,2)}^{(1)} & w_{k(d,3)}^{(1)} & ... & w_{k(d,n)}^{(1)}\\\\\\\n",
    "    ... ... ... \\\\\\\n",
    "    w_{k(1,1)}^{(h)} & w_{k(1,2)}^{(h)} & w_{k(1,3)}^{(h)} & ... & w_{k(1,n)}^{(h)}\\\\\\ \n",
    "    w_{k(2,1)}^{(h)} & w_{k(2,2)}^{(h)} & w_{k(2,3)}^{(h)} & ... & w_{k(2,n)}^{(h)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{k(d,1)}^{(h)} & w_{k(d,2)}^{(h)} & w_{k(d,3)}^{(h)} & ... & w_{k(d,n)}^{(h)}\\\\\\\n",
    "    w_{v(1,1)}^{(1)} & w_{v(1,2)}^{(1)} & w_{v(1,3)}^{(1)} & ... & w_{v(1,n)}^{(1)}\\\\\\ \n",
    "    w_{v(2,1)}^{(1)} & w_{v(2,2)}^{(1)} & w_{v(2,3)}^{(1)} & ... & w_{v(2,n)}^{(1)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{v(p,1)}^{(1)} & w_{v(p,2)}^{(1)} & w_{v(p,3)}^{(1)} & ... & w_{v(p,n)}^{(1)}\\\\\\\n",
    "    ... ... ... \\\\\\\n",
    "    w_{v(1,1)}^{(h)} & w_{v(1,2)}^{(h)} & w_{v(1,3)}^{(h)} & ... & w_{v(1,n)}^{(h)}\\\\\\ \n",
    "    w_{v(2,1)}^{(h)} & w_{v(2,2)}^{(h)} & w_{v(2,3)}^{(h)} & ... & w_{v(2,n)}^{(h)}\\\\\\\n",
    "    ... \\\\\\ \n",
    "    w_{v(p,1)}^{(h)} & w_{v(p,2)}^{(h)} & w_{v(p,3)}^{(h)} & ... & w_{v(p,n)}^{(h)}\n",
    "\\end{pmatrix}  \n",
    "\\begin{pmatrix}\n",
    "    x_1^{'}\\\\\\ x_2^{'}\\\\\\ ... \\\\\\ x_n^{'}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "    q_1^{(1)} \\\\\n",
    "    q_2^{(1)} \\\\\n",
    "    ... \\\\\n",
    "    q_d^{(1)} \\\\\n",
    "    ....\\\\\n",
    "    q_1^{(h)} \\\\\n",
    "    q_2^{(h)} \\\\\n",
    "    ... \\\\\n",
    "    q_d^{(h)} \\\\\n",
    "    k_1^{(1)} \\\\\n",
    "    k_2^{(1)} \\\\\n",
    "    ... \\\\\n",
    "    k_d^{(1)} \\\\\n",
    "    ....\\\\\n",
    "    k_1^{(h)} \\\\\n",
    "    k_2^{(h)} \\\\\n",
    "    ... \\\\\n",
    "    k_d^{(h)} \\\\\n",
    "    v_1^{(1)} \\\\\n",
    "    v_2^{(1)} \\\\\n",
    "    ... \\\\\n",
    "    v_p^{(1)} \\\\\n",
    "    ....\\\\\n",
    "    v_1^{(h)} \\\\\n",
    "    v_2^{(h)} \\\\\n",
    "    ... \\\\\n",
    "    v_p^{(h)} \\\\\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "This head-long vector can be used to calculate a a head-long hidden repressentation:\n",
    "\n",
    "$ \n",
    "H^{heads} =\n",
    "\\begin{pmatrix}\n",
    "    H^{(1)} \\\\\n",
    "    H^{(2)} \\\\\n",
    "    ... \\\\\n",
    "    H^{(h)} \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "    V^{(1)}\\\\\\ \n",
    "    ...\\\\\\ \n",
    "    V^{(h)}\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "    A^{(1)} = [soft](arg)max(K^{T (1)}q^{(1)}) \\\\\\ \n",
    "    ...\\\\\\ \n",
    "    A^{(h)}  = [soft](arg)max(K^{T (h)}q^{(h)}) \\\\\\ \n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "    h_1^{(1)} \\\\\n",
    "    h_2^{(1)} \\\\\n",
    "    ... \\\\\n",
    "    h_p^{(1)} \\\\\n",
    "    ....\\\\\n",
    "    h_1^{(h)} \\\\\n",
    "    h_2^{(h)} \\\\\n",
    "    ... \\\\\n",
    "    h_p^{(h)} \\\\\n",
    "\\end{pmatrix} \n",
    "$\n",
    "€ $\\mathbb{R}^{hp}$\n",
    "\n",
    "> Don't confuse the `h` symbol to represent a value in the hidden representation vector with the `h` used to represent the number of heads. \n",
    "\n",
    "This head-long hidden repressentation vector can be converted to the desired dimensionality with a linear multiplication.\n",
    "\n",
    "$\n",
    "W_h ($ € $\\mathbb{R}^{pxhp})\n",
    "\\begin{pmatrix}\n",
    "    h_1^{(1)} \\\\\n",
    "    h_2^{(1)} \\\\\n",
    "    ... \\\\\n",
    "    h_p^{(1)} \\\\\n",
    "    ....\\\\\n",
    "    h_1^{(h)} \\\\\n",
    "    h_2^{(h)} \\\\\n",
    "    ... \\\\\n",
    "    h_p^{(h)} \\\\\n",
    "\\end{pmatrix}  = H^{'} $  € $\\mathbb{R}^{p}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder-decoder architecture. \n",
    "\n",
    "This is the same architecture of `auto-encoders`:\n",
    "\n",
    "![auto-encoder](res/autoencoder.png)\n",
    "\n",
    "In transformers we have something similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer encoder\n",
    "\n",
    "![transformer_encoder](res/transformers_encoder.png)\n",
    "\n",
    "Where:\n",
    " - `self-attention` layer as we explained before.\n",
    " - `1-convolution` layer is called too `feed-forward` but at the end is a convolutional layer with kernel size 1. Is a linear layer apply to every element in the set.\n",
    " - `Add, norm` layer is composed by 2 component: 1) addition component and then 2) a layer normalization.\n",
    " \n",
    " ![transformers_add_norm](./res/transformers_add_norm.png)\n",
    " \n",
    " - Both the `self-attention` as the `1-convolutional` part have a residual connexion. \n",
    "\n",
    "In this encoder we insert the input set ($\\{x^{(i)}\\}_{i=1}^{t} = \\{x^{(1)}, x^{(2)}, ..., x^{(t)}\\}$) and at the end we get the hidden representation of the output of the encoder ($\\{h^{Enc(i)}\\}_{i=1}^{t} = \\{h^{Enc(1)}, h^{Enc(2)}, ..., h^{Enc(t)}\\}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer decoder\n",
    "\n",
    "Decoder is similar to the decoder but it includes a new part between the `self-attention` and the `1-convolution` part, the `cross-attention` component.\n",
    "\n",
    "![transformer_decoder](res/transformers_decoder.png)\n",
    "\n",
    "The input ($x$) of the `cross-attention` for the keys ($k$) and values ($v$) is the hidden representation of the encoder:\n",
    "\n",
    "$k = W_kh^{Enc(i)}$ ; $v = W_vh^{Enc(i)}$\n",
    "\n",
    "Meanwhile, for the query ($q$) the input is the output of the `self-attention` module:\n",
    "\n",
    "$q = W_qx^{satt}$ \n",
    "\n",
    "In this transformer decoder, in a auto-regresion fashion, the outupt is going to be the input in the next step of the encoder.\n",
    "\n",
    "$\\{y^{(i)}\\}_{i=0}^{t-1} = \\{0, h_1^{Dec}, , h_2^{Dec}, , h_3^{Dec}, ..., , h_{t-1}^{Dec} \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the encoder summarise the input in the hidden representation. Then the decoder queries what is require throw the $q$ from the set of representations of the encoder ($k$, $v$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL NOTES\n",
    "\n",
    " - Transformes allows make all the computation in only one iteration (no sequential operations). In constrat, other models as RNN take the input set item by item, in consequence, it is a sequencial of operations. Transformer allows you parallelize the process: $H = VA$\n",
    " \n",
    " - In transformers, we have a problem with the size of the matrix $A$ € $\\mathbb{R}^{(txt)}$. We have to limit `t` to avoid the explosion of weights in the model.\n",
    " \n",
    " - Recommended readings:\n",
    "   - http://jalammar.github.io/illustrated-transformer/ (the matrix are in horizontal but the computation is the same).\n",
    "   - https://distill.pub/2016/augmented-rnns/ (see the attention illustrations).\n",
    "   - https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html (there are some error but it's a good explanation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicum\n",
    "\n",
    "In this notebook we are going to create an transformer encoder and the we are going to train this encoder to classify texts. In particular, we want classify movie reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_Softargmax = nn.Softmax  # fix wrong name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi head attention\n",
    "\n",
    "[Theory section](#Implementation)\n",
    "\n",
    "When we consider `h` heads. The output vector is the concatenation of the `h` matrixes/vectors of $q$, $k$ and $v$:\n",
    "\n",
    "![cross-attetion-heads-imp](res/transformers_cross_attention_imp_heads_eq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, p, d_input=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        if d_input is None:\n",
    "            d_xq = d_xk = d_xv = d_model\n",
    "        else:\n",
    "            d_xq, d_xk, d_xv = d_input\n",
    "            \n",
    "        # Make sure that the embedding dimension of model is a multiple of number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.d_k = d_model // self.num_heads\n",
    "        \n",
    "        # These are still of dimension d_model. They will be split into number of heads \n",
    "        # - This matrix allows us to rotate the current input (see section: #Queries,-keys-and-values)\n",
    "        self.W_q = nn.Linear(d_xq, d_model, bias=False)  # q = W_q*x\n",
    "        self.W_k = nn.Linear(d_xk, d_model, bias=False)  # k = W_k*x\n",
    "        self.W_v = nn.Linear(d_xv, d_model, bias=False)  # v = W_v*x\n",
    "        \n",
    "        \n",
    "        # Outputs of all sub-layers need to be of dimension d_model\n",
    "        # -  (see section (in cross-attetion): #Implementation)\n",
    "        self.W_h = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        \"\"\"\n",
    "        Attention vectorization calculation: A = softargmax(K^T*Q) and\n",
    "        Hidden vectorization representation: H = V*A \n",
    "        (see section: #Cross-attention)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = Q.size(0) \n",
    "        k_length = K.size(-2) \n",
    "        \n",
    "        # Scaling by d_k so that the soft(arg)max doesnt saturate\n",
    "        Q = Q / np.sqrt(self.d_k)                           # (bs, n_heads, q_length, dim_per_head)\n",
    "        \n",
    "        # -- Compute the K - Q aligment: K^T*Q\n",
    "        scores = torch.matmul(Q, K.transpose(2,3))          # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        # -- Compute the attention: softargmax(K-Q aligment)\n",
    "        A = nn_Softargmax(dim=-1)(scores)   # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        # -- Compute the hidden representation: H = V*A\n",
    "        # Get the weighted average of the values\n",
    "        H = torch.matmul(A, V)     # (bs, n_heads, q_length, dim_per_head)\n",
    "\n",
    "        return H, A \n",
    "\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (heads X depth)\n",
    "        Return after transpose to put in shape (batch_size X num_heads X seq_length X d_k)\n",
    "        \"\"\"\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def group_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Combine the heads again to get (batch_size X seq_length X (num_heads times d_k))\n",
    "        \"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "    \n",
    "\n",
    "    def forward(self, X_q, X_k, X_v):\n",
    "        batch_size, seq_length, dim = X_q.size()\n",
    "\n",
    "        # -- Gets query Q, keys K and values V from the inputs using W_q, W_k and W_v\n",
    "        # After transforming, split into num_heads \n",
    "        Q = self.split_heads(self.W_q(X_q), batch_size)  # (bs, n_heads, q_length, dim_per_head) -> Q = W_q*X_q\n",
    "        K = self.split_heads(self.W_k(X_k), batch_size)  # (bs, n_heads, k_length, dim_per_head) -> K = W_k*X_k\n",
    "        V = self.split_heads(self.W_v(X_v), batch_size)  # (bs, n_heads, v_length, dim_per_head) -> V = W_v*X_v\n",
    "        \n",
    "        # -- Calculate the attention weights and hidden representations for each of the heads \n",
    "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Put all the heads back together by concat\n",
    "        H_cat = self.group_heads(H_cat, batch_size)    # (bs, q_length, dim)\n",
    "        \n",
    "        # -- Collapse the h-long hidden representation vector in the model dimension using W_h\n",
    "        # -  (see section (in cross-attetion): #Implementation)\n",
    "        # Final linear layer  \n",
    "        H = self.W_h(H_cat)          # (bs, q_length, dim)\n",
    "        \n",
    "        return H, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some sanity checks:\n",
    "\n",
    "Now we are going to do some tests to check if the attention process implemented in the previous class works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8, p=0)\n",
    "def print_out(Q, K, V):\n",
    "    temp_out, temp_attn = temp_mha.scaled_dot_product_attention(Q, K, V)\n",
    "    print('Attention weights are:', temp_attn.squeeze())\n",
    "    print('Output is:', temp_out.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check our self attention works - if the query matches with one of the key values, it should have all the attention focused there, with the value returned as the value at that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tensor([3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06])\n",
      "Output is: tensor([1.0004e+01, 4.0993e-05, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "test_K = torch.tensor(\n",
    "    [[10, 0, 0],\n",
    "     [ 0,10, 0],\n",
    "     [ 0, 0,10],\n",
    "     [ 0, 0,10]]\n",
    ").float()[None,None]\n",
    "\n",
    "test_V = torch.tensor(\n",
    "    [[   1,0,0],\n",
    "     [  10,0,0],\n",
    "     [ 100,5,0],\n",
    "     [1000,6,0]]\n",
    ").float()[None,None]\n",
    "\n",
    "test_Q = torch.tensor(\n",
    "    [[0, 10, 0]]\n",
    ").float()[None,None]\n",
    "\n",
    "# You can see the q is equal to the second key : [0 10 0]\n",
    "#  that implies => the attention should go to the second position\n",
    "#  that implies => the output should be the second value : [10 0 0]\n",
    "# Of course, the output is not exactly the value (because we use soft attention) but it is very close\n",
    "\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can see that it focuses on the second key and returns the second value. \n",
    "\n",
    "If we give a query that matches two keys exactly, it should return the averaged value of the two values for those two keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tensor([1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01])\n",
      "Output is: tensor([549.9979,   5.5000,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor([[0, 0, 10]]).float()  \n",
    "\n",
    "# You can see the q is equal to the third and fourth key : [0 0 10]\n",
    "#  that implies => the attention should go to the third and fourth position\n",
    "#  that implies => the output should be the average of both values : (1/2) ([100 5 0] + [1000 6 0]) = [550 5.5 0]\n",
    "\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it focuses equally on the third and fourth key and returns the average of their values.\n",
    "\n",
    "Now giving all the queries at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tensor([[1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01],\n",
      "        [3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06],\n",
      "        [5.0000e-01, 5.0000e-01, 1.8633e-06, 1.8633e-06]])\n",
      "Output is: tensor([[5.5000e+02, 5.5000e+00, 0.0000e+00],\n",
      "        [1.0004e+01, 4.0993e-05, 0.0000e+00],\n",
      "        [5.5020e+00, 2.0497e-05, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor(\n",
    "    [[0, 0, 10], [0, 10, 0], [10, 10, 0]]\n",
    ").float()[None,None]\n",
    "\n",
    "# Vectorization of the q's\n",
    "#   we obtain the output for 3 queries in only one iteration\n",
    "\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D convolution with `kernel_size = 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically an [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) with one hidden layer and ReLU activation applied to each and every element in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim, p):\n",
    "        super().__init__()\n",
    "        self.k1convL1 = nn.Linear(d_model,    hidden_dim)\n",
    "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.k1convL1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.k1convL2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all components for our Transformer Encoder block shown below!!!!\n",
    "\n",
    "\n",
    "![transformer_encoder](res/transformers_encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, conv_hidden_dim, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, p) # Self-attetion = Cross-attetion with q=k=v=x\n",
    "        self.cnn = CNN(d_model, conv_hidden_dim, p)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Self attention \n",
    "        attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Layer norm after adding the residual connection \n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Feed forward \n",
    "        cnn_output = self.cnn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        #Second layer norm after adding residual connection \n",
    "        out2 = self.layernorm2(out1 + cnn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Blocks of N Encoder Layers + Positional encoding + Input embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the encoder to text classification. \n",
    "\n",
    "As input we have a set of inputs $\\{x^{(i)}\\}_{i=1}^t = \\{x^{(1)},x^{(2)}, ... x^{(t)}\\}$, where $x^{(i)}$ € $\\mathbb{R}^{n}$\n",
    "\n",
    "![input-repres](res/transformers_xinput_embbd.png)\n",
    "\n",
    "The transofmer encoder and attention are permutation equivariant. In other words, they don't use the order of the inputs as feature. You can change the order of the input set and the result is gonna be the same.\n",
    "\n",
    "In contrast, to classify sentences, the position $i$ of the input $x^{(i)}$ matters. We need include information about what position the input item takes. \n",
    "\n",
    "Self attention by itself does not have any recurrence or convolutions so to make it sensitive to position we must provide additional positional encodings. These are calculated as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "E(p, 2i)    &= \\sin(p / 10000^{2i / d}) \\\\\n",
    "E(p, 2i+1) &= \\cos(p / 10000^{2i / d})\n",
    "\\end{aligned}\n",
    "\n",
    "@alfredo: the theorical concept is the important thing, how we include the positional encoding is only technicalities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
    "    theta = np.array([\n",
    "        [p / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "        for p in range(nb_p)\n",
    "    ])\n",
    "    E[:, 0::2] = torch.FloatTensor(np.sin(theta[:, 0::2]))\n",
    "    E[:, 1::2] = torch.FloatTensor(np.cos(theta[:, 1::2]))\n",
    "    E.detach_()\n",
    "    E.requires_grad = False\n",
    "    E = E.to(device)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, max_position_embeddings, p):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "        create_sinusoidal_embeddings(\n",
    "            nb_p=max_position_embeddings,\n",
    "            dim=d_model,\n",
    "            E=self.position_embeddings.weight\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) # (max_seq_length)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)                      # (bs, max_seq_length)\n",
    "        \n",
    "        # Get word embeddings for each input id\n",
    "        word_embeddings = self.word_embeddings(input_ids)                   # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Get position embeddings for each position id \n",
    "        position_embeddings = self.position_embeddings(position_ids)        # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Add them both \n",
    "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Layer norm \n",
    "        embeddings = self.LayerNorm(embeddings)             # (bs, max_seq_length, dim)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Neural Network\n",
    "\n",
    "We can stick several transformer encoders, one after other, to have a Deep Neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_hidden_dim, input_vocab_size,\n",
    "               maximum_position_encoding, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embeddings(d_model, input_vocab_size,maximum_position_encoding, p)\n",
    "\n",
    "        # We can stick different encoder, one after other, to have a Deep Neural network.\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.enc_layers.append(EncoderLayer(d_model, num_heads, ff_hidden_dim, p))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
    "\n",
    "        # We feed forward one transformer encoder after other\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train network to classify texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/.conda/envs/pDL/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/victor/.conda/envs/pDL/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/victor/.conda/envs/pDL/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  25000\n",
      "test :  25000\n",
      "train.fields : {'text': <torchtext.data.field.Field object at 0x7f3d4ecaaf70>, 'label': <torchtext.data.field.LabelField object at 0x7f3d4ecaafd0>}\n"
     ]
    }
   ],
   "source": [
    "max_len = 200\n",
    "text = data.Field(sequential=True, fix_length=max_len, batch_first=True, lower=True, dtype=torch.long)\n",
    "label = data.LabelField(sequential=False, dtype=torch.long)\n",
    "datasets.IMDB.download('./')\n",
    "ds_train, ds_test = datasets.IMDB.splits(text, label, path='./imdb/aclImdb/')\n",
    "print('train : ', len(ds_train))\n",
    "print('test : ', len(ds_test))\n",
    "print('train.fields :', ds_train.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  22500\n",
      "valid :  2500\n",
      "test :  25000\n"
     ]
    }
   ],
   "source": [
    "ds_train, ds_valid = ds_train.split(0.9)\n",
    "print('train : ', len(ds_train))\n",
    "print('valid : ', len(ds_valid))\n",
    "print('test : ', len(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 50_000\n",
    "text.build_vocab(ds_train, max_size=num_words)\n",
    "label.build_vocab(ds_train)\n",
    "vocab = text.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/.conda/envs/pDL/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 164\n",
    "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Composed network = List Encoders + Final layer to classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size, num_answers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size,\n",
    "                         maximum_position_encoding=10000)\n",
    "        self.dense = nn.Linear(d_model, num_answers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x, _ = torch.max(x, dim=1)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embeddings(\n",
       "      (word_embeddings): Embedding(50002, 32, padding_idx=1)\n",
       "      (position_embeddings): Embedding(10000, 32)\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (cnn): CNN(\n",
       "          (k1convL1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (k1convL2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerClassifier(num_layers=1, d_model=32, num_heads=2, \n",
    "                         conv_hidden_dim=128, input_vocab_size=50002, num_answers=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All the training processes have to include the following steps:\n",
    "\n",
    " 1. Forward process: `output = model(data)`\n",
    " 2. Get loss: `loss = criterion(output, target)`\n",
    " 3. Clear gradient buffers: `optimizer.zero_grad()`\n",
    " 4. Calculate gradient (the partial derivate of the loss with the respect the network paramenters): `loss.backward()`\n",
    " 5. Perform training setp (step in the oppositional direction of the gradient): `optimizer.step()`\n",
    "\n",
    "If some of these steps are missed the training is going to go wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "t_total = len(train_loader) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
    "        nb_batches_train = len(train_loader)\n",
    "        train_acc = 0\n",
    "        model.train()\n",
    "        losses = 0.0\n",
    "\n",
    "        for batch in train_iterator:\n",
    "            x = batch.text.to(device)\n",
    "            y = batch.label.to(device)\n",
    "            \n",
    "            # Perform the forward pass of the model\n",
    "            out = model(x)  # ①\n",
    "\n",
    "            # Get loss\n",
    "            loss = f.cross_entropy(out, y)  # ②\n",
    "            \n",
    "            # Clear gradient buffers\n",
    "            model.zero_grad()  # ③\n",
    "\n",
    "            # Calculate gradient \n",
    "            loss.backward()  # ④\n",
    "            losses += loss.item()\n",
    "\n",
    "            # Perform training setp \n",
    "            optimizer.step()  # ⑤\n",
    "                        \n",
    "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "        \n",
    "        print(f\"Training loss at epoch {epoch} is {losses / nb_batches_train}\")\n",
    "        print(f\"Training accuracy: {train_acc / nb_batches_train}\")\n",
    "        print('Evaluating on validation:')\n",
    "        evaluate(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    data_iterator = iter(data_loader)\n",
    "    nb_batches = len(data_loader)\n",
    "    model.eval()\n",
    "    acc = 0 \n",
    "    for batch in data_iterator:\n",
    "        x = batch.text.to(device)\n",
    "        y = batch.label.to(device)\n",
    "                \n",
    "        out = model(x)\n",
    "        acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "\n",
    "    print(f\"Eval accuracy: {acc / nb_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/.conda/envs/pDL/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 0 is 0.6701715005480725\n",
      "Training accuracy: 0.5873265729939904\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.6432545731707316\n",
      "Training loss at epoch 1 is 0.5970086343046548\n",
      "Training accuracy: 0.6884389360197948\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.7049542682926829\n",
      "Training loss at epoch 2 is 0.5117290296416351\n",
      "Training accuracy: 0.7567161541180628\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.7530868902439024\n",
      "Training loss at epoch 3 is 0.42599487995755847\n",
      "Training accuracy: 0.8094456963591377\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.7768673780487806\n",
      "Training loss at epoch 4 is 0.3614137342226678\n",
      "Training accuracy: 0.8439322640509015\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.7990853658536585\n",
      "Training loss at epoch 5 is 0.3108235130059546\n",
      "Training accuracy: 0.8714927978084124\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8073932926829268\n",
      "Training loss at epoch 6 is 0.2587649969087131\n",
      "Training accuracy: 0.8969158713326268\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.811623475609756\n",
      "Training loss at epoch 7 is 0.2187954171196274\n",
      "Training accuracy: 0.9162192912689998\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8158155487804878\n",
      "Training loss at epoch 8 is 0.17484821168624837\n",
      "Training accuracy: 0.9382290562036059\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.818140243902439\n",
      "Training loss at epoch 9 is 0.14181907906912375\n",
      "Training accuracy: 0.9539479498055845\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8138719512195122\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy: 0.8041274775492854\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
