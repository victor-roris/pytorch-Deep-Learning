{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: automatic differentiation\n",
    "\n",
    "The ``autograd`` package provides automatic differentiation for all operations\n",
    "on Tensors. It is a define-by-run framework, which means that your backprop is\n",
    "defined by how your code is run, and that every single iteration can be\n",
    "different.\n",
    "\n",
    "In the forward phase, the autograd tape will remember all the operations it executed, and in the backward phase, it will replay the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a 2x2 tensor with gradient-accumulation capabilities\n",
    "x = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > The param `requires_grad` makes the pytorch keeps record of all the gradiant computations over the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an operation on the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Deduct 2 from all elements\n",
    "y = x - 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SubBackward0 object at 0x7f2f9d332eb0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SubBackward refers that y was generated from a substraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but `x` is directly the same as it was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# What's happening here?\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> x wasn't modify, it hasn't any gradiant computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to analyse the gradient information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubBackward0 at 0x7f2f9d332eb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's dig further...\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AccumulateGrad at 0x7f2f9d32d730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can track to the orginal variable `x` origin of the `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to compute a new tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 3y^2 = \n",
      "tensor([[ 3.,  0.],\n",
      "        [ 3., 12.]], grad_fn=<MulBackward0>)\n",
      "\n",
      "a = 1/n * sumi(zi) (where n=2*2=4) =\n",
      "tensor(4.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3  # This is the same of 3y^2\n",
    "a = z.mean()   # average = 1/n * sumi(zi)\n",
    "\n",
    "print(\"z = 3y^2 = \")\n",
    "print(z)\n",
    "\n",
    "print()\n",
    "print(\"a = 1/n * sumi(zi) (where n=2*2=4) =\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent the gradient computation as a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise the computational graph! (thks @szagoruyko)\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"106pt\" height=\"271pt\"\n",
       " viewBox=\"0.00 0.00 106.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-267 102,-267 102,4 -4,4\"/>\n",
       "<!-- 139840329783184 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139840329783184</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"98,-21 0,-21 0,0 98,0 98,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 139840329783136 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139840329783136</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"94.5,-78 3.5,-78 3.5,-57 94.5,-57 94.5,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139840329783136&#45;&gt;139840329783184 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139840329783136&#45;&gt;139840329783184</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M49,-56.7787C49,-49.6134 49,-39.9517 49,-31.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.5001,-31.1732 49,-21.1732 45.5001,-31.1732 52.5001,-31.1732\"/>\n",
       "</g>\n",
       "<!-- 139840329783280 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139840329783280</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"94.5,-135 3.5,-135 3.5,-114 94.5,-114 94.5,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139840329783280&#45;&gt;139840329783136 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139840329783280&#45;&gt;139840329783136</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M49,-113.7787C49,-106.6134 49,-96.9517 49,-88.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.5001,-88.1732 49,-78.1732 45.5001,-88.1732 52.5001,-88.1732\"/>\n",
       "</g>\n",
       "<!-- 139842477567664 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139842477567664</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"94,-192 4,-192 4,-171 94,-171 94,-192\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 139842477567664&#45;&gt;139840329783280 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139842477567664&#45;&gt;139840329783280</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M43.5885,-170.7787C42.3317,-163.6134 41.9599,-153.9517 42.4733,-145.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"45.9738,-145.498 43.597,-135.1732 39.0164,-144.7267 45.9738,-145.498\"/>\n",
       "</g>\n",
       "<!-- 139842477567664&#45;&gt;139840329783280 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139842477567664&#45;&gt;139840329783280</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M54.4115,-170.7787C55.6683,-163.6134 56.0401,-153.9517 55.5267,-145.3097\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"58.9836,-144.7267 54.403,-135.1732 52.0262,-145.498 58.9836,-144.7267\"/>\n",
       "</g>\n",
       "<!-- 139842477545264 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139842477545264</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"76,-263 22,-263 22,-228 76,-228 76,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"49\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (2, 2)</text>\n",
       "</g>\n",
       "<!-- 139842477545264&#45;&gt;139842477567664 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139842477545264&#45;&gt;139842477567664</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M49,-227.6724C49,-219.8405 49,-210.5893 49,-202.4323\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.5001,-202.2234 49,-192.2234 45.5001,-202.2235 52.5001,-202.2234\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f2f1d2e9730>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have from top to down:\n",
    "\n",
    "    1. The original tensor `x` of dimension 2x2\n",
    "    2. The tensor 'y' generated by the sustraction over `x` \n",
    "    3. The y*y operation --> z_1 = y^2\n",
    "    4. The multiplication by 3 over the previous calculated tensor --> z = 3*z_1\n",
    "    5. Finally, the mean operation applied over all the values of the previous tensor --> a = 1/4 * sumi(zi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "Let's backprop now `out.backward()` is equivalent to doing `out.backward(torch.tensor([1.0]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manual computation of the gradient of the previous sequence is:\n",
    "\n",
    "![gradiente computation](./res/gradient_notebook3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check that the gradient calculated by pytorch is equal to our calculated gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop\n",
    "a.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients $\\frac{\\text{d}a}{\\text{d}x}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5000,  0.0000],\n",
      "        [ 1.5000,  3.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Compute it by hand BEFORE executing this\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! It`s the same gradient value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do many crazy things with autograd!\n",
    "> With Great *Flexibility* Comes Great Responsibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.8306,  0.3325, -0.7245], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Dynamic graphs!\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 937.2817,  170.2165, -370.9525], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x * 2\n",
    "i = 0\n",
    "\n",
    "#  vertor x norm = sqrt(x1**2 + x2**2 + ... + xn**2)\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i += 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([512., 512., 512.])\n"
     ]
    }
   ],
   "source": [
    "# If we don't run backward on a scalar we need to specify the grad_output\n",
    "# gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "gradients = torch.FloatTensor([1.0, 1.0, 1.0])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How manys iterations `i` have been executed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "y = 2^(i+1) * x\n",
    "\n",
    "dy/dx = 2^(i+1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log2(x.grad[1])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# BEFORE executing this, can you tell what would you expect it to print?\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the input and weight matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable decides the tensor's range below\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated matrix of input x: \n",
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "\n",
      "Generated matrix of weights w: \n",
      "tensor([1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Both x and w that allows gradient accumulation\n",
    "x = torch.arange(1., n + 1, requires_grad=True)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "\n",
    "print(\"Generated matrix of input x: \")\n",
    "print(x)\n",
    "\n",
    "print()\n",
    "print(\"Generated matrix of weights w: \")\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do linear regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = w @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we compute gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x :\n",
      "tensor([1., 1., 1.])\n",
      "\n",
      "Gradient of w :\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "\n",
    "print(\"Gradient of x :\")\n",
    "print(x.grad)\n",
    "\n",
    "print()\n",
    "print(\"Gradient of w :\")\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dz/dx = w\n",
    "\n",
    "dz/dw = x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we repite the same process but we don't allow pytorch to keep traking of gradient computation of `x` input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x :\n",
      "None\n",
      "\n",
      "Gradient of w :\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Only w that allows gradient accumulation\n",
    "x = torch.arange(1., n + 1)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "z = w @ x\n",
    "z.backward()\n",
    "\n",
    "print(\"Gradient of x :\")\n",
    "print(x.grad)\n",
    "\n",
    "print()\n",
    "print(\"Gradient of w :\")\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we compute the linear regression without allow gradient computation (`with torch.no_grad()`) it causes exception when we compute backpropatation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError!!! >:[\n",
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1., n + 1)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "\n",
    "# Regardless of what you do in this context, all torch tensors will not have gradient accumulation\n",
    "with torch.no_grad():\n",
    "    z = w @ x\n",
    "\n",
    "try:\n",
    "    z.backward()  # PyTorch will throw an error here, since z has no grad accum.\n",
    "except RuntimeError as e:\n",
    "    print('RuntimeError!!! >:[')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More stuff\n",
    "\n",
    "Documentation of the automatic differentiation package is at\n",
    "http://pytorch.org/docs/autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pDL] *",
   "language": "python",
   "name": "conda-env-.conda-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
